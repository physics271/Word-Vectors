{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "0. If you haven't already, follow [the setup instructions here](https://jennselby.github.io/MachineLearningCourseNotes/#setting-up-python3) to get all necessary software installed.\n",
    "0. Install the Gensim word2vec Python implementation: `python3 -m pip install --upgrade gensim`\n",
    "0. Get the trained model (1billion_word_vectors.zip) from Canvas and put it in the same folder as this ipynb file.\n",
    "0. Unzip the trained model file. You should now have three files in the folder (if zip created a new folder, move these files out of that separate folder into the same folder as this ipynb file):\n",
    "    * 1billion_word_vectors\n",
    "    * 1billion_word_vectors.syn1neg.npy\n",
    "    * 1billion_word_vectors.wv.syn0.npy\n",
    "0. Read through the code in the following sections:\n",
    "    * [Load trained word vectors](#Load-Trained-Word-Vectors)\n",
    "    * [Explore word vectors](#Explore-Word-Vectors)\n",
    "0. Optionally, complete [Exercise: Explore Word Vectors](#Exercise:-Explore-Word-Vectors)\n",
    "0. Read through the code in the following sections:\n",
    "    * [Use Word Vectors in an Embedding Layer of a Keras Model](#Use-Word-Vectors-in-an-Embedding-Layer-of-a-Keras-Model)\n",
    "    * [IMDB Dataset](#IMDB-Dataset)\n",
    "    * [Train IMDB Word Vectors](#Train-IMDB-Word-Vectors)\n",
    "    * [Process Dataset](#Process-Dataset)\n",
    "    * [Classification With Word Vectors Trained With Model](#Classification-With-Word-Vectors-Trained-With-Model)\n",
    "0. Complete one of the two [Exercises](#Exercises). Remember to keep notes about what you do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Details -- Do Not Do This\n",
    "This took awhile, which is why I'm giving you the trained file rather than having you do this. But just in case you're curious, here is how to create the trained model file.\n",
    "1. Download the corpus of sentences from [http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz](http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz)\n",
    "1. Unzip and unarchive the file: `tar zxf 1-billion-word-language-modeling-benchmark-r13output.tar.gz` \n",
    "1. Run the following Python code:\n",
    "    ```\n",
    "    from gensim.models import word2vec\n",
    "    import os\n",
    "\n",
    "    corpus_dir = '1-billion-word-language-modeling-benchmark-r13output/training-monolingual.tokenized.shuffled'\n",
    "    sentences = word2vec.PathLineSentences(corpus_dir)\n",
    "    model = word2vec.Word2Vec(sentences) # just use all of the default settings for now\n",
    "    model.save('1billion_word_vectors')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation/Sources\n",
    "* [https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html) for more information about how to use gensim word2vec in general\n",
    "* _Blog post has been removed_ [https://codekansas.github.io/blog/2016/gensim.html](https://codekansas.github.io/blog/2016/gensim.html) for information about using it to create embedding layers for neural networks.\n",
    "* [https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/) for information on sequence classification with keras\n",
    "* [https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) for using pre-trained embeddings with keras (though the syntax they use for the model layers is different than most other tutorials).\n",
    "* [https://keras.io/](https://keras.io/) Keras API documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model file into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = word2vec.Word2Vec.load('1billion_word_vectors/1billion_word_vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not need to continue training the model, we can save memory by keeping the parts we need (the word vectors themselves) and getting rid of the rest of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec = wv_model.wv\n",
    "del wv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Word Vectors\n",
    "Now we can look at some of the relationships between different words.\n",
    "\n",
    "Like [the gensim documentation](https://radimrehurek.com/gensim/models/word2vec.html), let's start with a famous example: king + woman - man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8407387733459473),\n",
       " ('monarch', 0.7541723251342773),\n",
       " ('prince', 0.7350203394889832),\n",
       " ('princess', 0.696908175945282),\n",
       " ('empress', 0.677180290222168),\n",
       " ('sultan', 0.6649758815765381),\n",
       " ('Chakri', 0.6451102495193481),\n",
       " ('goddess', 0.6439394950866699),\n",
       " ('ruler', 0.6275453567504883),\n",
       " ('kings', 0.6273428201675415)]"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one does not work as well as I'd hoped, but it gets close. Maybe you can find a better example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('okapi', 0.7140712738037109),\n",
       " ('gibbon', 0.7034620046615601),\n",
       " ('koala', 0.697202742099762),\n",
       " ('cub', 0.6907659769058228),\n",
       " ('tortoise', 0.6886162757873535),\n",
       " ('beetle', 0.6859476566314697),\n",
       " ('salamander', 0.6855185031890869),\n",
       " ('psyllid', 0.6837549209594727),\n",
       " ('lynx', 0.6802860498428345),\n",
       " ('carnivore', 0.6794542670249939)]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.most_similar(positive=['panda', 'eucalyptus'], negative=['bamboo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one of these is not like the others?\n",
    "\n",
    "Note: It looks like the gensim code needs to be updated to meet the requirements of later versions of numpy. You can ignore the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'laptop'"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.doesnt_match(['red', 'purple', 'laptop', 'turquoise', 'ruby'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How far apart are different words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.205414  , 0.36557418, 0.6597437 ], dtype=float32)"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.distances('laptop', ['computer', 'phone', 'rabbit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what one of these vectors actually looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50756323, -2.8890731 ,  0.9743826 , -0.60089743, -0.23762947,\n",
       "       -2.324566  , -0.64634913, -0.66476715, -2.3432739 ,  1.4446437 ,\n",
       "       -0.15542823,  1.8248576 ,  1.1309539 , -0.21071543, -0.82512087,\n",
       "       -0.2773584 , -0.1973424 , -0.5337731 ,  2.1143918 ,  1.0673765 ,\n",
       "       -0.2341243 ,  1.5292411 ,  0.66977274,  1.1214821 , -0.57710004,\n",
       "       -0.02504024,  0.6074397 ,  0.19416903, -1.1265849 , -0.6618393 ,\n",
       "        1.7525213 ,  1.6232891 , -0.3886833 , -1.1867149 ,  0.45511633,\n",
       "        1.4240934 , -0.87929034, -1.8920534 ,  2.6986032 , -0.5277589 ,\n",
       "        2.1202435 ,  0.62670445,  1.0352231 ,  1.4998924 ,  2.5809426 ,\n",
       "        0.74698585, -0.07757699, -0.67074645,  1.6887746 , -0.22081567,\n",
       "        1.2107906 ,  0.16741815,  3.3496742 ,  1.1832954 ,  0.4423463 ,\n",
       "        0.04771314, -0.14557275, -1.3345221 ,  1.3236852 ,  2.0154989 ,\n",
       "       -0.6510446 ,  0.21808812, -0.31578887, -1.822629  ,  0.8436349 ,\n",
       "       -1.1500564 ,  1.24044   , -2.6430037 ,  1.0617311 ,  1.2009143 ,\n",
       "        2.910486  ,  0.7534945 , -0.74546903, -0.12999983,  0.62368834,\n",
       "       -0.34848922,  0.9471761 ,  2.3520417 ,  2.1063364 , -1.3959917 ,\n",
       "        0.01752609,  0.92311406,  0.3009809 , -1.680638  ,  0.47537982,\n",
       "       -1.3736504 , -0.19134097,  1.03852   , -1.8218307 ,  1.7354008 ,\n",
       "        0.6050655 , -3.3719819 ,  0.3659331 , -0.71509886, -1.7725863 ,\n",
       "       -3.8122861 ,  2.9105155 , -0.89803743,  1.2961383 , -0.88507044],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec['textbook']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other methods are available to us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Explore Word Vectors\n",
    "\n",
    "## Optional\n",
    "What other interesting relationship can you find, using the methods used in the examples above or anything you find in the help message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pedometer', 0.6295429468154907),\n",
       " ('laptop', 0.6263337135314941),\n",
       " ('password', 0.6027147173881531),\n",
       " ('algorithm', 0.5952103137969971),\n",
       " ('scanner', 0.5935894250869751),\n",
       " ('timer', 0.5882337093353271),\n",
       " ('checklist', 0.5852446556091309),\n",
       " ('notepad', 0.5820284485816956),\n",
       " ('worksheet', 0.5760012865066528),\n",
       " ('stopwatch', 0.5734262466430664)]"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.most_similar(positive=['computer', 'calculator'], negative=['digital'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('atomic', 0.6709246635437012),\n",
       " ('reactor', 0.6436331272125244),\n",
       " ('thermonuclear', 0.6344550848007202),\n",
       " ('centrifuge', 0.6326947808265686),\n",
       " ('particle', 0.6218961477279663),\n",
       " ('hydrogen', 0.6185126304626465),\n",
       " ('smasher', 0.616949737071991),\n",
       " ('MOX', 0.6114870309829712),\n",
       " ('fission', 0.6085549592971802),\n",
       " ('furnace', 0.6038919687271118)]"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying to get \"neutron\"\n",
    "wordvec.most_similar(positive=['atom', 'proton'], negative=['electron'])\n",
    "#Doesn't really work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Word Vectors in an Embedding Layer of a Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed in the help text for wordvec that it has a built-in method for converting into a Keras embedding layer.\n",
    "\n",
    "Since for this experimentation, we'll just be giving the embedding layer one word at a time, we can set the input length to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedding_layer = wordvec.get_keras_embedding()\n",
    "test_embedding_layer.input_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Sequential()\n",
    "embedding_model.add(test_embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how do we actually use this? If you look at the [Keras Embedding Layer documentation](https://keras.io/layers/embeddings/) you might notice that it takes numerical input, not strings. How do we know which number corresponds to a particular word? In addition to having a vector, each word has an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30438"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec.vocab['python'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get the same vector from the embedding layer as we get from our word vector object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9039964 , -0.8687282 ,  0.3749935 , -0.6578742 ,  0.37530112,\n",
       "       -0.62705725,  0.5762422 ,  0.65298367,  0.78124946, -0.4666522 ,\n",
       "       -0.7576607 , -0.21010174, -0.9093117 ,  0.6348757 , -0.74911636,\n",
       "        0.39935622, -1.3958608 ,  2.6008337 ,  1.2817062 ,  1.4269804 ,\n",
       "       -0.82803494, -0.39129663,  1.3156073 , -0.6012138 , -2.078051  ,\n",
       "        0.23132282, -0.15733206, -1.2036172 , -1.4367007 ,  2.5016248 ,\n",
       "       -0.25131467, -0.61056453, -1.2893118 , -0.50152737,  0.46292958,\n",
       "        1.4078965 ,  0.32209706, -2.443665  , -0.07269833, -0.4449788 ,\n",
       "       -1.480729  , -0.3407248 ,  0.50580305,  1.5043081 , -1.931893  ,\n",
       "       -0.547079  , -0.37289718, -0.33272007, -1.4036684 ,  0.29593146,\n",
       "        3.15573   ,  0.8724582 , -1.0637406 ,  1.0174576 , -0.26892385,\n",
       "        1.0875573 , -0.13432768,  1.0217547 ,  1.4646659 , -0.1378838 ,\n",
       "       -0.8681816 , -0.2598224 ,  1.1302258 ,  0.46259895, -1.0285823 ,\n",
       "       -1.5687151 , -0.7058934 , -0.27440086, -0.48677507,  1.6343483 ,\n",
       "       -0.8900602 , -0.8832175 , -1.5720515 ,  0.8288372 ,  0.8799498 ,\n",
       "       -0.24864702, -0.49238694,  2.1108294 ,  1.4638537 ,  1.1950543 ,\n",
       "       -0.49083644,  1.2505679 ,  1.254264  , -2.0109708 ,  1.232334  ,\n",
       "        0.5216255 ,  0.6749972 ,  1.1875595 , -0.68064374,  0.77327275,\n",
       "        2.4226923 ,  1.0611398 , -1.5845419 ,  0.2853551 , -0.02969814,\n",
       "        1.8781508 ,  0.3633493 ,  2.3620942 , -0.3940647 , -0.65818644],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec['~']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x296faf4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.00363996, -0.13187036,  0.01277165, -0.09377816,\n",
       "          0.03488827,  0.10659633, -0.03508453, -0.02759439,\n",
       "          0.072306  , -0.01991057,  0.09685616,  0.02015782,\n",
       "         -0.00118342,  0.04388103,  0.06658366,  0.04337954,\n",
       "         -0.07360943,  0.05326826,  0.0279623 ,  0.03987852,\n",
       "         -0.01242836, -0.03311308, -0.00274042, -0.14553909,\n",
       "         -0.21119477,  0.04372618, -0.21264377, -0.06405832,\n",
       "          0.0613315 , -0.04734587,  0.0004796 ,  0.0035618 ,\n",
       "         -0.14236291,  0.07266693, -0.06887726,  0.1348201 ,\n",
       "          0.02804956, -0.28549907, -0.1764764 ,  0.0672286 ,\n",
       "         -0.12392736, -0.084198  ,  0.13360597,  0.06411812,\n",
       "         -0.09272418, -0.03500371, -0.13909446,  0.05210727,\n",
       "         -0.05069349,  0.06005209,  0.05058956,  0.03888933,\n",
       "         -0.13578868,  0.0301847 ,  0.03773663, -0.03327306,\n",
       "          0.03955114, -0.00895859, -0.0795022 , -0.06930766,\n",
       "          0.10551341, -0.00870985,  0.08283576, -0.10099767,\n",
       "          0.07288817, -0.05085361,  0.03743833,  0.02734717,\n",
       "          0.05699846,  0.16711515,  0.25596172, -0.0682174 ,\n",
       "         -0.05473528,  0.04506918,  0.05161908, -0.15294026,\n",
       "         -0.00666178,  0.14739887,  0.01509667,  0.0775693 ,\n",
       "         -0.17908655, -0.02642231,  0.04052621, -0.01458185,\n",
       "          0.03031654, -0.01166712, -0.05165214, -0.09367768,\n",
       "          0.07565127,  0.07334021,  0.12688884,  0.22398098,\n",
       "         -0.1500785 , -0.03253009,  0.12500122, -0.02659715,\n",
       "         -0.01161873,  0.10819086, -0.02456986,  0.00849189]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.predict(numpy.array([[552401]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, right? But let's not waste our time when the computer could tell us definitively and quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model.predict(numpy.array([[wordvec.vocab['python'].index]]))[0][0] == wordvec['python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way to turn words into word vectors with Keras layers. Yes! Time to get some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Dataset\n",
    "The [IMDB dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) consists of movie reviews that have been marked as positive or negative. (There is also a built-in dataset of [Reuters newswires](https://keras.io/datasets/#reuters-newswire-topics-classification) that have been classified by topic.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our labels consist of 0 or 1, which makes sense for positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 1 0 1]\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:9])\n",
    "print(max(y_train))\n",
    "print(min(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But x is a bit more trouble. The words have already been converted to numbers -- numbers that have nothing to do with the word embeddings we spent time learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the help page for imdb, it appears there is a way to get the word back. Phew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module keras.datasets.imdb in keras.datasets:\n",
      "\n",
      "NAME\n",
      "    keras.datasets.imdb - IMDB sentiment classification dataset.\n",
      "\n",
      "FILE\n",
      "    /usr/local/lib/python3.8/site-packages/keras/datasets/imdb.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_offset = 3\n",
    "imdb_map = dict((index + imdb_offset, word) for (word, index) in imdb.get_word_index().items())\n",
    "imdb_map[0] = 'PADDING'\n",
    "imdb_map[1] = 'START'\n",
    "imdb_map[2] = 'UNKNOWN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The knowledge about the initial indices and offset came from [this stack overflow post](https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset) after I got gibberish when I tried to translate the first review, below. It looks coherent now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"START this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([imdb_map[word_index] for word_index in x_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train IMDB Word Vectors\n",
    "The word vectors from the 1 billion words dataset might work for us when trying to classify the IMDB data. Word vectors trained on the IMDB data itself might work better, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [['PADDING'] + [imdb_map[word_index] for word_index in review] for review in x_train]\n",
    "test_sentences = [['PADDING'] + [imdb_map[word_index] for word_index in review] for review in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min count says to put any word that appears at least once into the vocabulary\n",
    "# size sets the dimension of the output vectors\n",
    "imdb_wv_model = word2vec.Word2Vec(train_sentences + test_sentences + ['UNKNOWN'], min_count=1, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_wordvec = imdb_wv_model.wv\n",
    "del imdb_wv_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset\n",
    "For this exercise, we're going to keep all inputs the same length (we'll see how to do variable-length later). This means we need to choose a maximum length for the review, cutting off longer ones and adding padding to shorter ones. What should we make the length? Let's understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest review: 2697 Shortest review: 70\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(review) for review in x_train + x_test]\n",
    "print('Longest review: {} Shortest review: {}'.format(max(lengths), min(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2697 words! Wow. Well, let's see how many reviews would get cut off at a particular cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8485 reviews out of 25000 are over 500.\n"
     ]
    }
   ],
   "source": [
    "cutoff = 500\n",
    "print('{} reviews out of {} are over {}.'.format(\n",
    "    sum([1 for length in lengths if length > cutoff]), \n",
    "    len(lengths), \n",
    "    cutoff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "x_train_padded = sequence.pad_sequences(x_train, maxlen=cutoff)\n",
    "x_test_padded = sequence.pad_sequences(x_test, maxlen=cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification With Word Vectors Trained With Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, Dense, Flatten, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model definition. The embedding layer here learns the 100-dimensional vector embedding within the overall classification problem training. That is usually what we want, unless we have a bunch of un-tagged data that could be used to train word vectors but not a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pretrained_model = Sequential()\n",
    "not_pretrained_model.add(a)\n",
    "not_pretrained_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "not_pretrained_model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "not_pretrained_model.add(Flatten())\n",
    "not_pretrained_model.add(Dense(units=128, activation='relu'))\n",
    "not_pretrained_model.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "not_pretrained_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. __This takes awhile. You might not want to re-run it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 86s 216ms/step - loss: 0.2788 - binary_accuracy: 0.8603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18d9c65b0>"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_pretrained_model.fit(x_train_padded, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the model. __This takes awhile. You might not want to re-run it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 13ms/step - loss: 0.2980 - binary_accuracy: 0.8826\n",
      "loss: 0.298037588596344 accuracy: 0.8826000094413757\n"
     ]
    }
   ],
   "source": [
    "not_pretrained_scores = not_pretrained_model.evaluate(x_test_padded, y_test)\n",
    "print('loss: {} accuracy: {}'.format(*not_pretrained_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## These exercises will help you learn more about how to use word vectors in a model and how to translate between data representations.\n",
    "\n",
    "## For any model that you try in these exercises, take notes about the performance you see and anything you notice about the differences between the models.\n",
    "\n",
    "## Exercise Option #1 - Advanced Difficulty\n",
    "Using the details above about how the imdb dataset and the keras embedding layer represent words, define a model that uses the pre-trained word vectors from the imdb dataset rather than an embedding that keras learns as it goes along. You'll need to replace the embedding layer and feed in different training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_num_to_index=numpy.vectorize(lambda x:imdb_wordvec.vocab[(imdb_map[x])].index)\n",
    "\n",
    "imdb_x_train_padded=imdb_num_to_index(x_train_padded)\n",
    "imdb_x_test_padded=imdb_num_to_index(x_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_embedding_layer_weights = imdb_wordvec.get_keras_embedding()._initial_weights\n",
    "imdb_input_dim = imdb_wordvec.get_keras_embedding().input_dim\n",
    "\n",
    "imdb_embedding_layer=Embedding(input_dim=imdb_input_dim, output_dim=100, input_length=500,\n",
    "                              weights=imdb_embedding_layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeli = Sequential()\n",
    "modeli.add(imdb_embedding_layer)\n",
    "modeli.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "modeli.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "modeli.add(Flatten())\n",
    "modeli.add(Dense(units=128 ,activation='relu'))\n",
    "modeli.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "modeli.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 70s 177ms/step - loss: 0.8467 - binary_accuracy: 0.6729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x191c2cdc0>"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeli.fit(imdb_x_train_padded, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3120 - binary_accuracy: 0.8660\n",
      "loss: 0.3119727373123169 accuracy: 0.8660399913787842\n"
     ]
    }
   ],
   "source": [
    "modeli_score = modeli.evaluate(imdb_x_test_padded, y_test)\n",
    "print('loss: {} accuracy: {}'.format(*modeli_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the pretrained embedding did result in a lower final accuracy, however it did cause the training time to be a good bit shorter (~16 seconds). The lower accuracy might be because the pretrained embedding was not necessarily as optimal for the specific task of deciding if a review is positive or negative as compared to whatever embedding keras learned from scratch. On the other hand, having a pretrained embedding did seem to result in keras needing to do less computation for every training generation, potentially because the model was already closer to a local optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Option #2 - Advanced Difficulty\n",
    "Same as option 1, but try using the 1billion vector word embeddings instead of the imdb vectors. If you also did option 1, comment on how the performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train_words=[]\n",
    "\n",
    "for review in x_train:\n",
    "    review_words=[imdb_map[index] for index in review]\n",
    "    review_indices=[wordvec.vocab[word].index for word in review_words if word in wordvec.vocab]\n",
    "    b_train_words.append(review_indices)\n",
    "    \n",
    "    \n",
    "b_test_words=[]\n",
    "\n",
    "for review in x_test:\n",
    "    review_words=[imdb_map[index] for index in review]\n",
    "    review_indices=[wordvec.vocab[word].index for word in review_words if word in wordvec.vocab]\n",
    "    b_test_words.append(review_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33817 is the index of 'padding'\n",
    "b_train_padded = sequence.pad_sequences(b_train_words, maxlen=cutoff, value=33817)\n",
    "b_test_padded = sequence.pad_sequences(b_test_words, maxlen=cutoff, value=33817)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "billion_embedding_layer_weights = wordvec.get_keras_embedding()._initial_weights\n",
    "billion_input_dim = wordvec.get_keras_embedding().input_dim\n",
    "\n",
    "billion_embedding_layer=Embedding(input_dim=billion_input_dim, output_dim=100, input_length=500,\n",
    "                              weights=billion_embedding_layer_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelb = Sequential()\n",
    "modelb.add(billion_embedding_layer)\n",
    "modelb.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "modelb.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "modelb.add(Flatten())\n",
    "modelb.add(Dense(units=128 ,activation='relu'))\n",
    "modelb.add(Dense(units=1, activation='sigmoid')) # because at the end, we want one yes/no answer\n",
    "modelb.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 300s 764ms/step - loss: 1.0006 - binary_accuracy: 0.5368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18b4840d0>"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelb.fit(b_train_padded, y_train, epochs=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 10s 12ms/step - loss: 0.4142 - binary_accuracy: 0.8127\n",
      "loss: 0.4142071604728699 accuracy: 0.8126800060272217\n"
     ]
    }
   ],
   "source": [
    "modelb_score = modelb.evaluate(b_test_padded, y_test)\n",
    "print('loss: {} accuracy: {}'.format(*modelb_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar vein as the IMDB pretrained embedding, using the 1billion pretrained embedding did result in a lower final accuracy, probably for the same reasons. However, using the 1billion pretrained embedding massively increased the amount of time needed for the model to finish training, mostly because using the 1billion pretrained embedding introduced a colossal amount of additional nodes on the first layer, thereby massively increasing the number of weights (and hence computations) that keras needed to perform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
